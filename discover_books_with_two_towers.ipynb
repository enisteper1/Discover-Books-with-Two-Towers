{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "from typing import Dict, Text\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import hnswlib\n",
    "\n",
    "# min rating to consider\n",
    "min_rating = 7\n",
    "\n",
    "# parameters\n",
    "output_dimension = 64\n",
    "batch_size = 256\n",
    "learning_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/b29gfxwj3tb43t74fmwm28nmrsc95x/T/ipykernel_21432/2153888385.py:2: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  books = pd.read_csv(\"dataset/Books.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Datasets\n",
    "books = pd.read_csv(\"dataset/Books.csv\")\n",
    "\n",
    "ratings = pd.read_csv(\"dataset/Ratings.csv\")\n",
    "\n",
    "users = pd.read_csv(\"dataset/Users.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization and Type Standardization\n",
    "users[\"User-ID\"] = users[\"User-ID\"].apply(lambda x: f\"user_{x}\")\n",
    "\n",
    "# Filter out books with missing or corrupted information\n",
    "books[\"ISBN\"] = books[\"ISBN\"].apply(lambda x: f\"book_{x}\")\n",
    "books.drop([\"Image-URL-S\", \"Image-URL-M\", \"Image-URL-L\"], axis=1, inplace=True)\n",
    "books.dropna(inplace=True)\n",
    "\n",
    "def clean_year(year):\n",
    "    try:\n",
    "        return int(year)\n",
    "    except ValueError:\n",
    "        return -1\n",
    "\n",
    "def surpress_year(year):\n",
    "    if year > max_year_boundary:\n",
    "        return max_year_boundary\n",
    "    elif year < min_year_boundary:\n",
    "        return min_year_boundary\n",
    "    \n",
    "    return year\n",
    "\n",
    "books['Year-Of-Publication'] = books['Year-Of-Publication'].apply(clean_year)\n",
    "books = books[books['Year-Of-Publication'] != -1].reset_index(drop=True)\n",
    "min_year_boundary = books['Year-Of-Publication'].mean() - books['Year-Of-Publication'].std()\n",
    "max_year_boundary = books['Year-Of-Publication'].mean() + books['Year-Of-Publication'].std()\n",
    "books['Year-Of-Publication'] = books['Year-Of-Publication'].apply(surpress_year)\n",
    "books = books.drop_duplicates(subset='Book-Title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings[\"ISBN\"] = ratings[\"ISBN\"].apply(lambda x: f\"book_{x}\")\n",
    "ratings[\"User-ID\"] = ratings[\"User-ID\"].apply(lambda x: f\"user_{x}\")\n",
    "ratings[\"Book-Rating\"] = ratings[\"Book-Rating\"].apply(lambda x: float(x))\n",
    "ratings = ratings[ratings.ISBN.isin(books['ISBN'].unique())]\n",
    "# Filtering products for simplicity\n",
    "\n",
    "# Only consider high ratings\n",
    "ratings = ratings[ratings[\"Book-Rating\"] >= min_rating]\n",
    "# Remove outlier users\n",
    "outlier_threshold = ratings['User-ID'].value_counts().quantile(0.9)\n",
    "user_rating_count_dict = ratings['User-ID'].value_counts().to_dict()\n",
    "ratings['rate_count'] = ratings['User-ID'].map(user_rating_count_dict)\n",
    "ratings = ratings[ratings['rate_count'] <= outlier_threshold]\n",
    "\n",
    "# Consider user & book that has rating in ratings dataset\n",
    "books = books[books.ISBN.isin(ratings['ISBN'].unique())]\n",
    "users = users[users['User-ID'].isin(ratings['User-ID'].unique())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book_0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>book_0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>HarperPerennial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book_0374157065</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>book_0399135782</td>\n",
       "      <td>The Kitchen God's Wife</td>\n",
       "      <td>Amy Tan</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>Putnam Pub Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>book_1558746218</td>\n",
       "      <td>A Second Chicken Soup for the Woman's Soul (Ch...</td>\n",
       "      <td>Jack Canfield</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>Health Communications</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ISBN                                         Book-Title  \\\n",
       "1   book_0002005018                                       Clara Callan   \n",
       "2   book_0060973129                               Decision in Normandy   \n",
       "3   book_0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       "5   book_0399135782                             The Kitchen God's Wife   \n",
       "14  book_1558746218  A Second Chicken Soup for the Woman's Soul (Ch...   \n",
       "\n",
       "             Book-Author  Year-Of-Publication              Publisher  \n",
       "1   Richard Bruce Wright               2001.0  HarperFlamingo Canada  \n",
       "2           Carlo D'Este               1991.0        HarperPerennial  \n",
       "3       Gina Bari Kolata               1999.0   Farrar Straus Giroux  \n",
       "5                Amy Tan               1991.0       Putnam Pub Group  \n",
       "14         Jack Canfield               1998.0  Health Communications  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Book to Book Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Groups: 16055\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_100004</td>\n",
       "      <td>[book_0345339703, book_0399146652, book_043906...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_10003</td>\n",
       "      <td>[book_068483068X, book_0743446593]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_100035</td>\n",
       "      <td>[book_0440211727, book_0671759310]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_100053</td>\n",
       "      <td>[book_0312422156, book_0316769487, book_038549...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_100066</td>\n",
       "      <td>[book_0060953713, book_0385722206, book_039309...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       User-ID                                          ISBN_list\n",
       "0  user_100004  [book_0345339703, book_0399146652, book_043906...\n",
       "1   user_10003                 [book_068483068X, book_0743446593]\n",
       "2  user_100035                 [book_0440211727, book_0671759310]\n",
       "3  user_100053  [book_0312422156, book_0316769487, book_038549...\n",
       "4  user_100066  [book_0060953713, book_0385722206, book_039309..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group books which are read from same user\n",
    "book_groups_raw = ratings.groupby('User-ID')\n",
    "book_groups = pd.DataFrame(\n",
    "    data={\n",
    "        \"User-ID\": list(book_groups_raw.groups.keys()),\n",
    "        \"ISBN_list\": list(book_groups_raw.ISBN.apply(list)),\n",
    "    }\n",
    ")\n",
    "# Eliminate if user has read one book\n",
    "book_groups = book_groups[book_groups['ISBN_list'].apply(len) > 1].reset_index(drop=True)\n",
    "print(f\"Number of Groups: {book_groups.shape[0]}\")\n",
    "book_groups.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Matches: 90940\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_ISBN</th>\n",
       "      <th>similar_ISBN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>book_0345339703</td>\n",
       "      <td>book_0399146652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book_0345339703</td>\n",
       "      <td>book_0439064872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>book_0345339703</td>\n",
       "      <td>book_059035342X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book_0399146652</td>\n",
       "      <td>book_0439064872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>book_0399146652</td>\n",
       "      <td>book_059035342X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         main_ISBN     similar_ISBN\n",
       "0  book_0345339703  book_0399146652\n",
       "1  book_0345339703  book_0439064872\n",
       "2  book_0345339703  book_059035342X\n",
       "3  book_0399146652  book_0439064872\n",
       "4  book_0399146652  book_059035342X"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "book_matches = []\n",
    "# for each book in our isbn_list we generate pairs\n",
    "for isbn_list in book_groups['ISBN_list'].values:\n",
    "    if len(isbn_list) <= 1:\n",
    "        continue\n",
    "    for i, main_isbn in enumerate(isbn_list[:-1]):\n",
    "        for similar_isbn in isbn_list[i+1:]:\n",
    "            book_matches.append([main_isbn, similar_isbn])\n",
    "\n",
    "# Dataset generation and visualization\n",
    "book_pairs_dataset = pd.DataFrame(book_matches, columns=[\"main_ISBN\", \"similar_ISBN\"])\n",
    "data_size = book_pairs_dataset.shape[0]\n",
    "print(f\"Number of Matches: {data_size}\")\n",
    "book_pairs_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_ISBN</th>\n",
       "      <th>similar_ISBN</th>\n",
       "      <th>main_Book-Title</th>\n",
       "      <th>main_Book-Author</th>\n",
       "      <th>main_Year-Of-Publication</th>\n",
       "      <th>main_Publisher</th>\n",
       "      <th>similar_Book-Title</th>\n",
       "      <th>similar_Book-Author</th>\n",
       "      <th>similar_Year-Of-Publication</th>\n",
       "      <th>similar_Publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>book_0345339703</td>\n",
       "      <td>book_0399146652</td>\n",
       "      <td>The Fellowship of the Ring (The Lord of the Ri...</td>\n",
       "      <td>J.R.R. TOLKIEN</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>Del Rey</td>\n",
       "      <td>The Cat Who Smelled a Rat</td>\n",
       "      <td>Lilian Jackson Braun</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Putnam Publishing Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book_0316284955</td>\n",
       "      <td>book_0399146652</td>\n",
       "      <td>White Oleander : A Novel (Oprah's Book Club)</td>\n",
       "      <td>Janet Fitch</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>Back Bay Books</td>\n",
       "      <td>The Cat Who Smelled a Rat</td>\n",
       "      <td>Lilian Jackson Braun</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Putnam Publishing Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>book_0312278586</td>\n",
       "      <td>book_0399146652</td>\n",
       "      <td>The Nanny Diaries: A Novel</td>\n",
       "      <td>Emma McLaughlin</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>St. Martin's Press</td>\n",
       "      <td>The Cat Who Smelled a Rat</td>\n",
       "      <td>Lilian Jackson Braun</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Putnam Publishing Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book_0316666343</td>\n",
       "      <td>book_0399146652</td>\n",
       "      <td>The Lovely Bones: A Novel</td>\n",
       "      <td>Alice Sebold</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>Little, Brown</td>\n",
       "      <td>The Cat Who Smelled a Rat</td>\n",
       "      <td>Lilian Jackson Braun</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Putnam Publishing Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>book_0140293248</td>\n",
       "      <td>book_0399146652</td>\n",
       "      <td>The Girls' Guide to Hunting and Fishing</td>\n",
       "      <td>Melissa Bank</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>Penguin Books</td>\n",
       "      <td>The Cat Who Smelled a Rat</td>\n",
       "      <td>Lilian Jackson Braun</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Putnam Publishing Group</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         main_ISBN     similar_ISBN  \\\n",
       "0  book_0345339703  book_0399146652   \n",
       "1  book_0316284955  book_0399146652   \n",
       "2  book_0312278586  book_0399146652   \n",
       "3  book_0316666343  book_0399146652   \n",
       "4  book_0140293248  book_0399146652   \n",
       "\n",
       "                                     main_Book-Title main_Book-Author  \\\n",
       "0  The Fellowship of the Ring (The Lord of the Ri...   J.R.R. TOLKIEN   \n",
       "1       White Oleander : A Novel (Oprah's Book Club)      Janet Fitch   \n",
       "2                         The Nanny Diaries: A Novel  Emma McLaughlin   \n",
       "3                          The Lovely Bones: A Novel     Alice Sebold   \n",
       "4            The Girls' Guide to Hunting and Fishing     Melissa Bank   \n",
       "\n",
       "   main_Year-Of-Publication      main_Publisher         similar_Book-Title  \\\n",
       "0                    1986.0             Del Rey  The Cat Who Smelled a Rat   \n",
       "1                    2000.0      Back Bay Books  The Cat Who Smelled a Rat   \n",
       "2                    2002.0  St. Martin's Press  The Cat Who Smelled a Rat   \n",
       "3                    2002.0       Little, Brown  The Cat Who Smelled a Rat   \n",
       "4                    2000.0       Penguin Books  The Cat Who Smelled a Rat   \n",
       "\n",
       "    similar_Book-Author  similar_Year-Of-Publication        similar_Publisher  \n",
       "0  Lilian Jackson Braun                       2001.0  Putnam Publishing Group  \n",
       "1  Lilian Jackson Braun                       2001.0  Putnam Publishing Group  \n",
       "2  Lilian Jackson Braun                       2001.0  Putnam Publishing Group  \n",
       "3  Lilian Jackson Braun                       2001.0  Putnam Publishing Group  \n",
       "4  Lilian Jackson Braun                       2001.0  Putnam Publishing Group  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final dataset to train our model \n",
    "# Main book features\n",
    "main_books = books.rename(columns=lambda x: 'main_' + x if x != 'ISBN' else x).copy()\n",
    "book_pairs = pd.merge(book_pairs_dataset, main_books,\n",
    "                              left_on='main_ISBN',\n",
    "                              right_on='ISBN')\n",
    "book_pairs.drop(\"ISBN\", axis=1, inplace=True)\n",
    "\n",
    "# Similar book features\n",
    "similar_books = books.rename(columns=lambda x: 'similar_' + x if x != 'ISBN' else x).copy()\n",
    "book_pairs = pd.merge(book_pairs, similar_books,\n",
    "                              left_on='similar_ISBN',\n",
    "                              right_on='ISBN')\n",
    "\n",
    "book_pairs.drop(\"ISBN\", axis=1, inplace=True)\n",
    "book_pairs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Convert Dataset to TFDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "# Pairs dataset\n",
    "book_pairs_final = tf.data.Dataset.from_tensor_slices({\n",
    "    # main book features\n",
    "    'main_ISBN': tf.cast(book_pairs['main_ISBN'], dtype=tf.string),\n",
    "    'main_Book-Title': tf.cast(book_pairs['main_Book-Title'], dtype=tf.string),\n",
    "    'main_Book-Author': tf.cast(book_pairs['main_Book-Author'], dtype=tf.string),\n",
    "    'main_Year-Of-Publication': tf.cast(book_pairs['main_Year-Of-Publication'], dtype=tf.int32),\n",
    "    'main_Publisher': tf.cast(book_pairs['main_Publisher'], dtype=tf.string),\n",
    "\n",
    "    # similar book features\n",
    "    'similar_ISBN': tf.cast(book_pairs['similar_ISBN'], dtype=tf.string),\n",
    "    'similar_Book-Title': tf.cast(book_pairs['similar_Book-Title'], dtype=tf.string),\n",
    "    'similar_Book-Author': tf.cast(book_pairs['similar_Book-Author'], dtype=tf.string),\n",
    "    'similar_Year-Of-Publication': tf.cast(book_pairs['similar_Year-Of-Publication'], dtype=tf.int32),\n",
    "    'similar_Publisher': tf.cast(book_pairs['similar_Publisher'], dtype=tf.string),\n",
    "})\n",
    "\n",
    "book_pairs_final = book_pairs_final.batch(batch_size)\n",
    "# Book information dataset\n",
    "book_infos = tf.data.Dataset.from_tensor_slices({\n",
    "    'ISBN': tf.cast(books['ISBN'], dtype=tf.string),\n",
    "    'Book-Title': tf.cast(books['Book-Title'], dtype=tf.string),\n",
    "    'Book-Author': tf.cast(books['Book-Author'], dtype=tf.string),\n",
    "    'Year-Of-Publication': tf.cast(books['Year-Of-Publication'], dtype=tf.int32),\n",
    "    'Publisher': tf.cast(books['Publisher'], dtype=tf.string),\n",
    "})\n",
    "book_infos = book_infos.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1002)\n",
    "train_percentage = 0.8\n",
    "batch_count = (data_size + batch_size - 1) // batch_size\n",
    "shuffled = book_pairs_final.shuffle(batch_count, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(int(batch_count * train_percentage))\n",
    "test = shuffled.skip(int(batch_count * train_percentage)).take(int(batch_count * (1 - train_percentage)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Book to Book Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique ISBN 41743\n",
      "Number of unique Book-Title 41743\n",
      "Number of unique Book-Author 21935\n",
      "Number of unique Year-Of-Publication 80\n",
      "Number of unique Publisher 5234\n"
     ]
    }
   ],
   "source": [
    "for col in books.columns:\n",
    "    print(f\"Number of unique {col} {books[col].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Book Title Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    }
   ],
   "source": [
    "def batch_distilbert_embeddings(book_titles):\n",
    "    # convert bytes to string\n",
    "    book_titles = [title.decode('utf-8') for title in book_titles.numpy()]\n",
    "    # set inputs\n",
    "    input_ids = tokenizer(book_titles, padding=True, truncation=True, return_tensors='pt', max_length=20)\n",
    "    # generate embeddings\n",
    "    outputs = bert_model(input_ids['input_ids'])\n",
    "    # get last layer\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "    # mean vector is required and also padded values should be excluded\n",
    "    input_mask = tf.cast(input_ids['attention_mask'], tf.float32)\n",
    "    input_mask_expanded = tf.expand_dims(input_mask, -1)\n",
    "    sum_embeddings = tf.reduce_sum(last_hidden_state.detach().numpy() * input_mask_expanded, axis=1)\n",
    "    sum_mask = tf.reduce_sum(input_mask_expanded, axis=1)\n",
    "    mean_embeddings = sum_embeddings / sum_mask\n",
    "    return book_titles, mean_embeddings\n",
    "\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "title_list = []\n",
    "title_embeddings = []\n",
    "\n",
    "# Process batches of book titles to get embeddings\n",
    "for batch in book_infos:\n",
    "    book_titles, embeddings = batch_distilbert_embeddings(batch['Book-Title'])\n",
    "    title_list.extend(book_titles)\n",
    "    title_embeddings.extend(embeddings)\n",
    "\n",
    "title_embeddings_df = pd.DataFrame({'Book-Title':title_list, 'embedding':title_embeddings})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Book ISBN, Author, Publisher Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Book Title Model\n",
    "title_output_dim = len(title_embeddings[0])\n",
    "# An embedding if there are any unknown text input occurs\n",
    "unknown_embedding = np.random.uniform(-1, 1, size=title_output_dim)\n",
    "title_embeddings.insert(0, unknown_embedding)\n",
    "book_title_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=title_list, mask_token=None),\n",
    "  tf.keras.layers.Embedding(\n",
    "                    input_dim=len(title_list) + 1,\n",
    "                    output_dim=title_output_dim,\n",
    "                    embeddings_initializer=tf.keras.initializers.Constant(np.vstack(title_embeddings)),\n",
    "                    trainable=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Book ID Model\n",
    "book_embedding_dimension = 64\n",
    "unique_book_ids = books['ISBN'].unique()\n",
    "book_id_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_book_ids, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_book_ids) + 1, book_embedding_dimension)\n",
    "])\n",
    "\n",
    "\n",
    "# Author Model\n",
    "author_embedding_dimension = 32\n",
    "unique_book_authors = books['Book-Author'].unique()\n",
    "book_author_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_book_authors, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_book_authors) + 1, author_embedding_dimension)\n",
    "])\n",
    "\n",
    "# Publisher Model\n",
    "publisher_embedding_dimension = 16\n",
    "unique_book_publishers = books['Publisher'].unique()\n",
    "book_publisher_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_book_publishers, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_book_publishers) + 1, publisher_embedding_dimension)\n",
    "])\n",
    "\n",
    "# Publication Year Layer\n",
    "book_year_layer = tf.keras.layers.Normalization(\n",
    "    axis=None\n",
    ")\n",
    "book_year_layer.adapt(book_infos.map(lambda x: x[\"Year-Of-Publication\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       "array([ 0.0026858 , -0.03741129, -0.01967944, -0.00474253,  0.04196215,\n",
       "        0.03489442, -0.02625586,  0.01576375, -0.01148453, -0.04737226,\n",
       "        0.00830209,  0.01122935, -0.00154225,  0.04320015,  0.01335922,\n",
       "        0.00305701,  0.04329654,  0.03451772,  0.04371735, -0.01457602,\n",
       "       -0.04764173,  0.00885023,  0.00981665, -0.01966577, -0.04971072,\n",
       "       -0.01737161,  0.01679437,  0.01647678, -0.02727261,  0.02590995,\n",
       "       -0.01437413, -0.04865533], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author Embedding Visualization\n",
    "book_author_model('Richard Bruce Wright')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Book Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, book_id_model, book_title_model, book_author_model, book_publisher_model, book_year_layer):\n",
    "    super().__init__()\n",
    "    # assigning sub models to convert ids to embeddings\n",
    "    self.book_id_model = book_id_model\n",
    "    self.book_title_model = book_title_model\n",
    "    self.book_author_model = book_author_model\n",
    "    self.book_publisher_model = book_publisher_model\n",
    "    self.book_year_layer = book_year_layer\n",
    "\n",
    "  def call(self, features: Dict[Text, tf.Tensor]):\n",
    "    \n",
    "    # concatenation of embeddings\n",
    "    return tf.concat([\n",
    "        self.book_id_model(features[\"ISBN\"]),\n",
    "        self.book_title_model(features['Book-Title']),\n",
    "        self.book_author_model(features[\"Book-Author\"]),\n",
    "        self.book_publisher_model(features[\"Publisher\"]),\n",
    "        tf.expand_dims(self.book_year_layer(features['Year-Of-Publication']), axis=1)\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 881])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialization of our book model\n",
    "book_model = BookModel(book_id_model,\n",
    "                       book_title_model,\n",
    "                       book_author_model,\n",
    "                       book_publisher_model,\n",
    "                       book_year_layer)\n",
    "\n",
    "# Sample example\n",
    "book_model({'Book-Author': ['Richard Bruce Wright'],\n",
    "            'Book-Title': ['Clara Callan'],\n",
    "            'ISBN': ['book_0002005018'],\n",
    "            'Publisher': ['HarperFlamingo Canada'],\n",
    "            'Year-Of-Publication': [2001]}).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics & Task\n",
    "metrics = tfrs.metrics.FactorizedTopK(candidates=book_infos.map(lambda features: book_model(features)))\n",
    "task = tfrs.tasks.Retrieval(metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Book to Book Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Book2BookModel(tfrs.Model):\n",
    "    def __init__(self, book_id_model, book_title_model, book_author_model, book_publisher_model, book_year_layer, output_dimension=64):\n",
    "        super().__init__()\n",
    "        self.book_id_model = book_id_model\n",
    "        self.book_title_model = book_title_model\n",
    "        self.book_author_model = book_author_model\n",
    "        self.book_publisher_model = book_publisher_model\n",
    "        self.book_year_layer = book_year_layer\n",
    "        # combining book model with output dimension to fix output dimension\n",
    "        self.book_model_raw = BookModel(self.book_id_model,\n",
    "                                        self.book_title_model,\n",
    "                                        self.book_author_model,\n",
    "                                        self.book_publisher_model,\n",
    "                                        self.book_year_layer)\n",
    "        self.book_model = tf.keras.Sequential([self.book_model_raw,\n",
    "                                               tf.keras.layers.Dense(output_dimension)])\n",
    "        # Metrics & Task\n",
    "        self.candidates = book_infos.map(lambda x: self.book_model(x))\n",
    "        metrics = tfrs.metrics.FactorizedTopK(candidates=self.candidates)\n",
    "        # negative sampling also applied\n",
    "        self.task = tfrs.tasks.Retrieval(metrics=metrics,\n",
    "                                        num_hard_negatives=5)\n",
    "    \n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=True):\n",
    "        # Generation of main book embedding from main item features\n",
    "        main_book_embedding = self.book_model({'ISBN':features['main_ISBN'],\n",
    "                                               'Book-Title':features['main_Book-Title'],\n",
    "                                               'Book-Author': features['main_Book-Author'],\n",
    "                                               'Publisher': features['main_Publisher'],\n",
    "                                               'Year-Of-Publication':features['main_Year-Of-Publication']})\n",
    "\n",
    "        # Generation of similar book embedding from similar item features\n",
    "        similar_book_embedding = self.book_model({'ISBN':features['similar_ISBN'],\n",
    "                                                  'Book-Title':features['similar_Book-Title'],\n",
    "                                                  'Book-Author': features['similar_Book-Author'],\n",
    "                                                  'Publisher': features['similar_Publisher'],\n",
    "                                                  'Year-Of-Publication':features['similar_Year-Of-Publication']})\n",
    "\n",
    "        # loss and the metric calculation\n",
    "        # compute metrics set false to skyrock training speed\n",
    "        return self.task(main_book_embedding,\n",
    "                         similar_book_embedding,\n",
    "                         compute_metrics=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Book to Book Model initialization\n",
    "book2book_model = Book2BookModel(book_id_model,\n",
    "                                 book_title_model,\n",
    "                                 book_author_model,\n",
    "                                 book_publisher_model,\n",
    "                                 book_year_layer)\n",
    "book2book_model.compile(optimizer=tf.keras.optimizers.legacy.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "284/284 [==============================] - 8s 19ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 5572.2999 - regularization_loss: 0.0000e+00 - total_loss: 5572.2999\n",
      "Epoch 2/10\n",
      "284/284 [==============================] - 6s 19ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 615.1133 - regularization_loss: 0.0000e+00 - total_loss: 615.1133\n",
      "Epoch 3/10\n",
      "284/284 [==============================] - 6s 19ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 521.2133 - regularization_loss: 0.0000e+00 - total_loss: 521.2133\n",
      "Epoch 4/10\n",
      "284/284 [==============================] - 6s 19ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 494.7285 - regularization_loss: 0.0000e+00 - total_loss: 494.7285\n",
      "Epoch 5/10\n",
      "284/284 [==============================] - 6s 19ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 480.4604 - regularization_loss: 0.0000e+00 - total_loss: 480.4604\n",
      "Epoch 6/10\n",
      "284/284 [==============================] - 6s 19ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 472.5241 - regularization_loss: 0.0000e+00 - total_loss: 472.5241\n",
      "Epoch 7/10\n",
      "284/284 [==============================] - 6s 19ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 466.8327 - regularization_loss: 0.0000e+00 - total_loss: 466.8327\n",
      "Epoch 8/10\n",
      "284/284 [==============================] - 6s 19ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 462.3470 - regularization_loss: 0.0000e+00 - total_loss: 462.3470\n",
      "Epoch 9/10\n",
      "284/284 [==============================] - 6s 19ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 458.8615 - regularization_loss: 0.0000e+00 - total_loss: 458.8615\n",
      "Epoch 10/10\n",
      "284/284 [==============================] - 6s 19ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 456.2865 - regularization_loss: 0.0000e+00 - total_loss: 456.2865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2c2ba6d60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book2book_model.fit(train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Embedding Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>book_0002005018</td>\n",
       "      <td>[0.118825436, 0.045262557, -0.09441517, 0.0824...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book_0060973129</td>\n",
       "      <td>[-0.16090837, -0.025830925, -0.012401611, 0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>book_0374157065</td>\n",
       "      <td>[-0.029113185, 0.29734114, -0.16034544, 0.0993...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book_0399135782</td>\n",
       "      <td>[0.122434825, 0.02598485, -0.001726985, -0.148...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>book_1558746218</td>\n",
       "      <td>[-0.18346615, -0.04044274, 0.062318973, 0.0636...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ISBN                                          embedding\n",
       "0  book_0002005018  [0.118825436, 0.045262557, -0.09441517, 0.0824...\n",
       "1  book_0060973129  [-0.16090837, -0.025830925, -0.012401611, 0.06...\n",
       "2  book_0374157065  [-0.029113185, 0.29734114, -0.16034544, 0.0993...\n",
       "3  book_0399135782  [0.122434825, 0.02598485, -0.001726985, -0.148...\n",
       "4  book_1558746218  [-0.18346615, -0.04044274, 0.062318973, 0.0636..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to apply the model to each book feature set and return ISBN with embeddings\n",
    "def extract_embeddings_with_isbn(features):\n",
    "    embeddings = book2book_model.book_model(features)\n",
    "    return features['ISBN'], embeddings\n",
    "\n",
    "# Mapping the function over the dataset\n",
    "book_embeddings = book_infos.map(extract_embeddings_with_isbn)\n",
    "isbn_list = []\n",
    "embeddings_list = []\n",
    "\n",
    "# Example to inspect or use the embeddings with ISBNs\n",
    "for isbn, embedding in book_embeddings:\n",
    "    isbn_list.extend(list(isbn.numpy().astype(str)))  \n",
    "    embeddings_list.extend(list(embedding.numpy()))\n",
    "\n",
    "book_embedding_dataset = pd.DataFrame({'ISBN':isbn_list, 'embedding':embeddings_list})\n",
    "book_embedding_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_embedding_dict = dict(zip(book_embedding_dataset.ISBN, book_embedding_dataset.embedding))\n",
    "book_title_dict = dict(zip(books['ISBN'], books['Book-Title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = output_dimension\n",
    "\n",
    "num_elements = book_embedding_dataset.shape[0]\n",
    "# hnswlib initialization with cosine similarity\n",
    "p = hnswlib.Index(space='cosine', dim=dim)\n",
    "\n",
    "p.init_index(max_elements=num_elements, ef_construction=100, M=16)\n",
    "\n",
    "p.set_ef(10)\n",
    "\n",
    "embeddings = np.vstack(book_embedding_dataset[\"embedding\"].values)\n",
    "p.add_items(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Similar Book Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_search(isbn, k=3):\n",
    "    \"\"\"Gets input embeddings and return top k similar items\"\"\"\n",
    "\n",
    "    # Generate embedding for the user query\n",
    "    query_embedding = book_embedding_dict[isbn]\n",
    "\n",
    "    if query_embedding is None:\n",
    "        return \"Invalid query or embedding generation failed.\"\n",
    "\n",
    "    labels, _ = p.knn_query(query_embedding, k=k+1)\n",
    "    results = book_embedding_dataset.iloc[list(labels[0][1:])].to_dict('records')\n",
    "    similar_isbns = [similar_isbn['ISBN'] for similar_isbn in results]\n",
    "    return similar_isbns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Book:\n",
      "Harry Potter and the Prisoner of Azkaban (Book 3)\n",
      "\n",
      "Similar Books:\n",
      "1. Harry Potter and the Sorcerer's Stone (Book 1)\n",
      "2. Harry Potter and the Order of the Phoenix (Book 5)\n",
      "3. The Dream Directory: The Comprehensive Guide to Analysis and Interpretation, With Explanations for More Than 350 Symbols and Theories\n",
      "4. Behind the Attic Wall (Avon Camelot Books (Paperback))\n",
      "5. Harry Potter and the Sorcerer's Stone (Harry Potter (Paperback))\n",
      "6. The Black Cauldron (Chronicles of Prydain (Paperback))\n",
      "7. Awaken from Death\n",
      "8. The Cooperman Variations (Benny Cooperman Mysteries (Paperback))\n",
      "9. The Wild Child\n",
      "10. BLAST FROM THE PAST : A NOVEL (Kinky Friedman Novels (Hardcover))\n"
     ]
    }
   ],
   "source": [
    "# Visualization of Recommendation\n",
    "# ISBN of \"Harry Potter and the Prisoner of Azkaban (Book 3)\"\n",
    "main_book = 'book_0439136350'\n",
    "print(f\"Main Book:\\n{book_title_dict[main_book]}\\n\\nSimilar Books:\")\n",
    "\n",
    "similar_books = book_search(main_book, k=10)\n",
    "for i, similar_book in enumerate(similar_books):\n",
    "    print(f\"{i+1}. {book_title_dict[similar_book]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Recommendations & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Highly Rated Popular Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popular Books:\n",
      "1. Anne of Avonlea (Anne of Green Gables Novels (Paperback))\n",
      "2. El Codigo Da Vinci / The Da Vinci Code\n",
      "3. Maus 1. Mein Vater kotzt Geschichte aus. Die Geschichte eines Ã?Â?berlebenden.\n",
      "4. Complete Chronicles of Narnia\n",
      "5. The Cat in the Hat\n",
      "6. The Ultimate Hitchhiker's Guide\n",
      "7. Dandelion Wine (Grand Master Editions)\n",
      "8. The Grapes of Wrath\n",
      "9. Der Alchimist.\n",
      "10. The Little Prince (Wordsworth Collection)\n"
     ]
    }
   ],
   "source": [
    "def get_popular_books(df_ratings, k=10):\n",
    "  # Calculate the number of ratings for each movie\n",
    "  rating_counts = df_ratings['ISBN'].value_counts().reset_index()\n",
    "  rating_counts.columns = ['ISBN', 'rating_count']\n",
    "\n",
    "  # Get the most frequently rated movies\n",
    "  min_ratings_threshold = rating_counts['rating_count'].quantile(0.95)\n",
    "\n",
    "  # Filter movies based on the minimum number of ratings\n",
    "  popular_movies = ratings.merge(rating_counts, on='ISBN')\n",
    "  popular_movies = popular_movies[popular_movies['rating_count'] >= min_ratings_threshold]\n",
    "\n",
    "  # Calculate the average rating for each movie\n",
    "  average_ratings = popular_movies.groupby('ISBN')['Book-Rating'].mean().reset_index()\n",
    "\n",
    "  # Get the top k rated movies\n",
    "  top_10_movies = list(average_ratings.sort_values('Book-Rating', ascending=False).head(k).ISBN.values)\n",
    "  return top_10_movies\n",
    "\n",
    "popular_books = get_popular_books(ratings)\n",
    "print(\"Popular Books:\")\n",
    "for i, popular_book in enumerate(popular_books):\n",
    "  print(f\"{i+1}. {book_title_dict[popular_book]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Similar Books "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply the model to each book feature set and return ISBN with embeddings\n",
    "def extract_pairs(features):\n",
    "    return features['main_ISBN'], features['similar_ISBN']\n",
    "\n",
    "main_isbn_list = []\n",
    "similar_isbn_list = []\n",
    "\n",
    "book_pairs_test = test.map(extract_pairs)\n",
    "for main_isbn, similar_isbn in book_pairs_test:\n",
    "    main_isbn_list.extend(list(main_isbn.numpy().astype(str)))  \n",
    "    similar_isbn_list.extend(list(similar_isbn.numpy().astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_reco_results = []\n",
    "two_tower_reco_results = []\n",
    "k = 50\n",
    "popular_books = get_popular_books(ratings, k=k)\n",
    "\n",
    "for main_isbn, similar_isbn in zip(main_isbn_list, similar_isbn_list):\n",
    "    popular_reco_check = np.isin(popular_books, similar_isbn).astype(int)\n",
    "    popular_reco_results.append(popular_reco_check)\n",
    "    # get embedding based recommendations\n",
    "    similar_books = book_search(main_isbn, k=k)\n",
    "    two_tower_check = np.isin(similar_books, similar_isbn).astype(int)\n",
    "    two_tower_reco_results.append(two_tower_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 NDCG Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two Tower NDCG result at top 3: 0.0112\n",
      "Popular recommendation NDCG result at top 3: 0.0\n",
      "\n",
      "\n",
      "Two Tower NDCG result at top 5: 0.0124\n",
      "Popular recommendation NDCG result at top 5: 0.0\n",
      "\n",
      "\n",
      "Two Tower NDCG result at top 10: 0.0138\n",
      "Popular recommendation NDCG result at top 10: 0.0\n",
      "\n",
      "\n",
      "Two Tower NDCG result at top 50: 0.017\n",
      "Popular recommendation NDCG result at top 50: 0.0014\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Since we have already sorted our recommendations\n",
    "# An array that represent our recommendation scores is used.\n",
    "representative_array = [[i for i in range(k, 0, -1)]] * len(two_tower_reco_results)\n",
    "\n",
    "for k in [3, 5, 10, 50]:\n",
    "  two_tower_result = ndcg_score(two_tower_reco_results,\n",
    "                                  representative_array, k=k)\n",
    "  popular_result = ndcg_score(popular_reco_results,\n",
    "                              representative_array, k=k)\n",
    "  \n",
    "  print(f\"Two Tower NDCG result at top {k}: {round(two_tower_result, 4)}\")\n",
    "  print(f\"Popular recommendation NDCG result at top {k}: {round(popular_result, 4)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- https://www.tensorflow.org/recommenders/examples/basic_retrieval\n",
    "- https://www.tensorflow.org/recommenders/examples/featurization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
